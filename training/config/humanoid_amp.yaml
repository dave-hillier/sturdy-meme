# AMP (Adversarial Motion Priors) training configuration
# For training a low-level controller (LLC) without latent space conditioning.

environment:
  num_envs: 4096
  sim_timestep: 0.016667  # 1/60
  sim_substeps: 2
  early_termination_height: 0.3
  max_episode_steps: 300

skeleton:
  path: data/characters/humanoid.glb

motions:
  manifest: data/calm/motions/manifest.yaml

# Policy network (LLC without style conditioning)
policy:
  obs_dim: auto       # computed from skeleton
  action_dim: auto     # computed from skeleton
  hidden_sizes: [1024, 512, 256]
  activation: relu
  log_std_init: -1.0

# Value network
value:
  hidden_sizes: [1024, 512, 256]
  activation: relu

# AMP discriminator
discriminator:
  hidden_sizes: [1024, 512]
  activation: relu

# PPO hyperparameters
ppo:
  learning_rate: 0.0003
  clip_epsilon: 0.2
  entropy_coeff: 0.01
  value_coeff: 0.5
  max_grad_norm: 1.0
  gamma: 0.99
  gae_lambda: 0.95
  num_epochs: 5
  num_minibatches: 4
  steps_per_epoch: 32

# AMP discriminator training
amp:
  learning_rate: 0.0001
  grad_penalty_weight: 10.0
  style_reward_weight: 0.5
  task_reward_weight: 0.5

# Task reward configuration
task:
  type: heading        # heading | location | strike
  target: [0, 0, 1]   # forward direction for heading task

# Training schedule
training:
  total_epochs: 3000
  checkpoint_interval: 50
  log_interval: 10
  output_dir: checkpoints/amp/
