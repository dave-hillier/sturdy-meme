// CALM batch MLP inference compute shader.
// Evaluates the LLC policy for multiple NPCs in parallel on the GPU.
//
// Architecture:
//   Per NPC:  styleMLP(latent) -> concat(styleEmbed, obs) -> mainMLP -> muHead -> actions
//
// Data layout:
//   - Weights are stored as flat SSBOs, with layer offsets precomputed on CPU
//   - Input: batched latent codes + observations (one row per NPC)
//   - Output: batched action vectors (one row per NPC)
//
// Each workgroup processes one NPC. Shared memory is used for the
// intermediate activations (ping-pong buffers).

#version 450

// Specialization constants for network dimensions
layout(constant_id = 0) const uint NUM_NPCS = 1;
layout(constant_id = 1) const uint LATENT_DIM = 64;
layout(constant_id = 2) const uint OBS_DIM = 102;
layout(constant_id = 3) const uint ACTION_DIM = 37;
layout(constant_id = 4) const uint MAX_HIDDEN = 1024;  // Largest hidden layer size

layout(local_size_x = 1) in;  // One NPC per invocation

// Push constants for layer configuration
layout(push_constant) uniform PushConstants {
    uint numLayers;          // Total number of MLP layers
    uint styleLayerCount;    // Number of style MLP layers
    uint mainLayerCount;     // Number of main MLP layers (including muHead)
    uint styleDim;           // Style embedding dimension (output of style MLP)
} pc;

// Weight storage: all layer weights packed sequentially
// Layout per layer: [inFeatures * outFeatures floats (weights)] [outFeatures floats (bias)]
// Layer metadata stored in layerMeta SSBO
layout(set = 0, binding = 0) readonly buffer WeightBuffer {
    float weights[];
};

// Layer metadata: [weightOffset, biasOffset, inFeatures, outFeatures, activation]
// activation: 0=none, 1=ReLU, 2=Tanh
layout(set = 0, binding = 1) readonly buffer LayerMeta {
    uint layerMeta[];  // 5 uints per layer
};

// Input: latent codes, one per NPC [NUM_NPCS x LATENT_DIM]
layout(set = 0, binding = 2) readonly buffer LatentBuffer {
    float latents[];
};

// Input: observations, one per NPC [NUM_NPCS x OBS_DIM]
layout(set = 0, binding = 3) readonly buffer ObsBuffer {
    float observations[];
};

// Output: actions, one per NPC [NUM_NPCS x ACTION_DIM]
layout(set = 0, binding = 4) writeonly buffer ActionBuffer {
    float actions[];
};

// Shared memory for intermediate activations (ping-pong)
shared float bufA[MAX_HIDDEN];
shared float bufB[MAX_HIDDEN];
shared float styleEmbed[MAX_HIDDEN];

// Read layer metadata
void getLayerMeta(uint layerIdx, out uint weightOff, out uint biasOff,
                  out uint inFeat, out uint outFeat, out uint activation) {
    uint base = layerIdx * 5;
    weightOff  = layerMeta[base + 0];
    biasOff    = layerMeta[base + 1];
    inFeat     = layerMeta[base + 2];
    outFeat    = layerMeta[base + 3];
    activation = layerMeta[base + 4];
}

// Forward one layer from bufA to bufB
void forwardLayerAtoB(uint weightOff, uint biasOff, uint inFeat, uint outFeat, uint activation) {
    for (uint j = 0; j < outFeat; ++j) {
        float sum = weights[biasOff + j];
        uint rowStart = weightOff + j * inFeat;
        for (uint k = 0; k < inFeat; ++k) {
            sum += weights[rowStart + k] * bufA[k];
        }
        if (activation == 1u) sum = max(sum, 0.0);       // ReLU
        else if (activation == 2u) sum = tanh(sum);       // Tanh
        bufB[j] = sum;
    }
}

// Forward one layer from bufB to bufA
void forwardLayerBtoA(uint weightOff, uint biasOff, uint inFeat, uint outFeat, uint activation) {
    for (uint j = 0; j < outFeat; ++j) {
        float sum = weights[biasOff + j];
        uint rowStart = weightOff + j * inFeat;
        for (uint k = 0; k < inFeat; ++k) {
            sum += weights[rowStart + k] * bufB[k];
        }
        if (activation == 1u) sum = max(sum, 0.0);       // ReLU
        else if (activation == 2u) sum = tanh(sum);       // Tanh
        bufA[j] = sum;
    }
}

void main() {
    uint npcIdx = gl_GlobalInvocationID.x;
    if (npcIdx >= NUM_NPCS) return;

    // --- Step 1: Load latent into bufA ---
    uint latentBase = npcIdx * LATENT_DIM;
    for (uint i = 0; i < LATENT_DIM; ++i) {
        bufA[i] = latents[latentBase + i];
    }

    // --- Step 2: Run style MLP (latent -> styleEmbed) ---
    bool useA = true;
    for (uint layer = 0; layer < pc.styleLayerCount; ++layer) {
        uint wOff, bOff, inF, outF, act;
        getLayerMeta(layer, wOff, bOff, inF, outF, act);

        if (useA) {
            forwardLayerAtoB(wOff, bOff, inF, outF, act);
        } else {
            forwardLayerBtoA(wOff, bOff, inF, outF, act);
        }
        useA = !useA;
    }

    // Copy style embedding result to styleEmbed buffer
    {
        uint lastStyleLayer = pc.styleLayerCount - 1;
        uint dummy1, dummy2, dummy3, styleDim, dummy4;
        getLayerMeta(lastStyleLayer, dummy1, dummy2, dummy3, styleDim, dummy4);
        for (uint i = 0; i < styleDim; ++i) {
            // After the loop, the result is in bufB if useA==false (even layers),
            // or bufA if useA==true (odd layers). useA was flipped after each layer,
            // so if useA is true now, the result is in bufB; if false, in bufA.
            // Actually: useA starts true, flips after each layer.
            // After 1 layer: useA=false, result in bufB
            // After 2 layers: useA=true, result in bufA
            // So: result is in bufA if useA==true, bufB if useA==false
            // Wait, no. useA=true means "next layer reads from A", meaning the
            // LAST layer wrote to B. So result is in the buffer we'd NOT read from.
            // useA==true  -> last wrote to B -> result in bufB
            // useA==false -> last wrote to A -> result in bufA
            styleEmbed[i] = useA ? bufB[i] : bufA[i];
        }
    }

    // --- Step 3: Concatenate styleEmbed + observation into bufA ---
    uint obsBase = npcIdx * OBS_DIM;
    for (uint i = 0; i < pc.styleDim; ++i) {
        bufA[i] = styleEmbed[i];
    }
    for (uint i = 0; i < OBS_DIM; ++i) {
        bufA[pc.styleDim + i] = observations[obsBase + i];
    }

    // --- Step 4: Run main MLP + muHead ---
    useA = true;
    for (uint layer = 0; layer < pc.mainLayerCount; ++layer) {
        uint globalLayer = pc.styleLayerCount + layer;
        uint wOff, bOff, inF, outF, act;
        getLayerMeta(globalLayer, wOff, bOff, inF, outF, act);

        if (useA) {
            forwardLayerAtoB(wOff, bOff, inF, outF, act);
        } else {
            forwardLayerBtoA(wOff, bOff, inF, outF, act);
        }
        useA = !useA;
    }

    // --- Step 5: Write output actions ---
    // Same logic: useA==true means result is in bufB, useA==false means bufA
    uint actionBase = npcIdx * ACTION_DIM;
    for (uint i = 0; i < ACTION_DIM; ++i) {
        actions[actionBase + i] = useA ? bufB[i] : bufA[i];
    }
}
