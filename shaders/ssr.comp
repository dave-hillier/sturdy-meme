#version 450

/*
 * ssr.comp - Screen-Space Reflections Compute Shader
 *
 * Implements hierarchical ray marching in screen space for reflections.
 * Based on "Stochastic Screen-Space Reflections" (SIGGRAPH 2015)
 *
 * Features:
 * - Linear ray marching with adaptive step size
 * - Binary search refinement for precise hit detection
 * - Temporal filtering for stability
 * - Edge fading to hide artifacts
 */

#extension GL_GOOGLE_include_directive : require

#include "bindings.glsl"

layout(local_size_x = 8, local_size_y = 8, local_size_z = 1) in;

// Input textures
layout(binding = BINDING_SSR_COLOR) uniform sampler2D colorTexture;
layout(binding = BINDING_SSR_DEPTH) uniform sampler2D depthTexture;

// Output SSR texture
layout(binding = BINDING_SSR_OUTPUT, rgba16f) uniform writeonly image2D ssrOutput;

// Previous frame SSR for temporal filtering
layout(binding = BINDING_SSR_PREV) uniform sampler2D prevSSR;

// Push constants
layout(push_constant) uniform SSRParams {
    mat4 viewMatrix;
    mat4 projMatrix;
    mat4 invViewMatrix;
    mat4 invProjMatrix;
    vec4 cameraPos;        // xyz = position, w = unused
    vec4 screenParams;     // xy = resolution, z = 1/width, w = 1/height
    float maxDistance;     // Maximum ray march distance
    float thickness;       // Depth thickness for hit detection
    float stride;          // Initial step size in pixels
    int maxSteps;          // Maximum ray march steps
    float fadeStart;       // Start fading at this fraction of max distance
    float fadeEnd;         // End fade at this fraction
    float temporalBlend;   // Blend with previous frame
    float padding;
};

// Reconstruct world position from depth
vec3 reconstructWorldPos(vec2 uv, float depth) {
    // Convert to NDC
    vec4 ndc = vec4(uv * 2.0 - 1.0, depth, 1.0);

    // Unproject to view space
    vec4 viewPos = invProjMatrix * ndc;
    viewPos /= viewPos.w;

    // Transform to world space
    vec4 worldPos = invViewMatrix * viewPos;
    return worldPos.xyz;
}

// Reconstruct view position from depth
vec3 reconstructViewPos(vec2 uv, float depth) {
    vec4 ndc = vec4(uv * 2.0 - 1.0, depth, 1.0);
    vec4 viewPos = invProjMatrix * ndc;
    return viewPos.xyz / viewPos.w;
}

// Project view position to screen UV
vec3 projectToScreen(vec3 viewPos) {
    vec4 clipPos = projMatrix * vec4(viewPos, 1.0);
    vec3 ndc = clipPos.xyz / clipPos.w;
    return vec3(ndc.xy * 0.5 + 0.5, ndc.z);
}

// Linear depth from hardware depth
float linearizeDepth(float d, float near, float far) {
    return near * far / (far - d * (far - near));
}

// Get view-space normal from depth (approximation)
vec3 getViewNormal(vec2 uv, vec2 texelSize) {
    float d0 = texture(depthTexture, uv).r;
    float d1 = texture(depthTexture, uv + vec2(texelSize.x, 0.0)).r;
    float d2 = texture(depthTexture, uv + vec2(0.0, texelSize.y)).r;

    vec3 p0 = reconstructViewPos(uv, d0);
    vec3 p1 = reconstructViewPos(uv + vec2(texelSize.x, 0.0), d1);
    vec3 p2 = reconstructViewPos(uv + vec2(0.0, texelSize.y), d2);

    return normalize(cross(p1 - p0, p2 - p0));
}

// Ray march through screen space
// Returns: xy = hit UV, z = hit confidence (0 = no hit, 1 = solid hit)
vec3 rayMarch(vec3 rayOrigin, vec3 rayDir, vec2 screenSize) {
    // Transform ray to view space
    vec3 viewRayOrigin = (viewMatrix * vec4(rayOrigin, 1.0)).xyz;
    vec3 viewRayDir = normalize((viewMatrix * vec4(rayDir, 0.0)).xyz);

    // Don't trace rays pointing toward camera
    if (viewRayDir.z > 0.0) {
        return vec3(0.0);
    }

    // Project start point
    vec3 startScreen = projectToScreen(viewRayOrigin);

    // Find end point by stepping along ray
    vec3 endViewPos = viewRayOrigin + viewRayDir * maxDistance;
    vec3 endScreen = projectToScreen(endViewPos);

    // Screen-space ray direction
    vec3 screenRayDir = endScreen - startScreen;
    float rayLength = length(screenRayDir.xy * screenSize);

    if (rayLength < 1.0) {
        return vec3(0.0);
    }

    // Normalize to step per pixel
    screenRayDir /= rayLength;

    // Step parameters
    float stepSize = stride;
    vec3 currentPos = startScreen;
    vec3 prevPos = currentPos;

    float hit = 0.0;
    vec2 hitUV = vec2(0.0);

    // Linear ray march
    for (int i = 0; i < maxSteps; i++) {
        prevPos = currentPos;
        currentPos += screenRayDir * stepSize;

        // Check bounds
        if (currentPos.x < 0.0 || currentPos.x > 1.0 ||
            currentPos.y < 0.0 || currentPos.y > 1.0 ||
            currentPos.z < 0.0 || currentPos.z > 1.0) {
            break;
        }

        // Sample depth
        float sampledDepth = texture(depthTexture, currentPos.xy).r;

        // Check for intersection
        float depthDiff = currentPos.z - sampledDepth;

        if (depthDiff > 0.0 && depthDiff < thickness) {
            // Binary search refinement
            vec3 refinedPos = currentPos;
            vec3 searchDir = screenRayDir * stepSize * 0.5;

            for (int j = 0; j < 4; j++) {
                searchDir *= 0.5;
                float testDepth = texture(depthTexture, refinedPos.xy).r;
                float testDiff = refinedPos.z - testDepth;

                if (testDiff > 0.0) {
                    refinedPos -= searchDir;
                } else {
                    refinedPos += searchDir;
                }
            }

            hitUV = refinedPos.xy;
            hit = 1.0;

            // Distance-based confidence
            float rayDist = length(refinedPos.xy - startScreen.xy) * rayLength / screenSize.x;
            float distFade = 1.0 - smoothstep(fadeStart * maxDistance, fadeEnd * maxDistance, rayDist);

            // Edge fading (near screen borders)
            vec2 edgeDist = min(hitUV, 1.0 - hitUV);
            float edgeFade = smoothstep(0.0, 0.1, min(edgeDist.x, edgeDist.y));

            hit *= distFade * edgeFade;
            break;
        }

        // Adaptive step size (increase as we go further)
        stepSize = stride * (1.0 + float(i) * 0.1);
    }

    return vec3(hitUV, hit);
}

void main() {
    ivec2 pixelCoord = ivec2(gl_GlobalInvocationID.xy);
    vec2 resolution = screenParams.xy;

    if (pixelCoord.x >= int(resolution.x) || pixelCoord.y >= int(resolution.y)) {
        return;
    }

    vec2 uv = (vec2(pixelCoord) + 0.5) / resolution;
    vec2 texelSize = 1.0 / resolution;

    // Sample depth at full resolution (we're at half res, so adjust UV)
    vec2 fullResUV = uv;
    float depth = texture(depthTexture, fullResUV).r;

    // Skip sky pixels
    if (depth >= 0.9999) {
        imageStore(ssrOutput, pixelCoord, vec4(0.0));
        return;
    }

    // Reconstruct world position
    vec3 worldPos = reconstructWorldPos(fullResUV, depth);

    // Get view-space normal (approximation from depth)
    vec3 viewNormal = getViewNormal(fullResUV, texelSize);

    // Transform normal to world space
    vec3 worldNormal = normalize((invViewMatrix * vec4(viewNormal, 0.0)).xyz);

    // Calculate reflection direction
    vec3 viewDir = normalize(worldPos - cameraPos.xyz);
    vec3 reflectDir = reflect(viewDir, worldNormal);

    // Perform ray march
    vec3 result = rayMarch(worldPos, reflectDir, resolution);

    vec4 reflectionColor = vec4(0.0);

    if (result.z > 0.0) {
        // Sample color at hit point
        vec3 hitColor = texture(colorTexture, result.xy).rgb;

        // Apply confidence
        reflectionColor = vec4(hitColor, result.z);
    }

    // Temporal filtering with previous frame
    vec4 prevColor = texture(prevSSR, uv);
    reflectionColor = mix(reflectionColor, prevColor, temporalBlend * 0.5);

    imageStore(ssrOutput, pixelCoord, reflectionColor);
}
